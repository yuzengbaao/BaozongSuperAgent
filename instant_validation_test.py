#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@File    : instant_validation_test.py  
@Time    : 2025å¹´07æœˆ19æ—¥ 19:35:00
@Author  : å®æ€»
@Version : 1.0
@Desc    : SuperAgentç¬é—´ä¿®å¤æ•ˆæœéªŒè¯æµ‹è¯•
"""

import asyncio
import sys
import time
import json
from typing import List, Dict
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from core.agent_core import BaozongSuperAgent


class AgentValidator:
    """Agentæ€§èƒ½éªŒè¯å™¨"""
    
    def __init__(self):
        self.agent = None
        self.test_results = []
        
    async def setup(self):
        """åˆå§‹åŒ–Agent"""
        print("ğŸš€ åˆå§‹åŒ–SuperAgent...")
        self.agent = BaozongSuperAgent("éªŒè¯æµ‹è¯•Agent")
        print("âœ… Agentå¯åŠ¨å®Œæˆ")
        
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "èƒ½åŠ›ä»‹ç»æµ‹è¯•",
                "query": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ çš„èƒ½åŠ›",
                "expected_keywords": ["èƒ½åŠ›", "è®°å¿†", "ä»»åŠ¡", "æŠ€æœ¯"],
                "min_length": 200,
                "importance": "high"
            },
            {
                "name": "æŠ€æœ¯é—®é¢˜æµ‹è¯•", 
                "query": "å¦‚ä½•ä¼˜åŒ–Pythonå¼‚æ­¥ç¼–ç¨‹çš„æ€§èƒ½ï¼Ÿ",
                "expected_keywords": ["å¼‚æ­¥", "æ€§èƒ½", "asyncio", "å¹¶å‘"],
                "min_length": 300,
                "importance": "high"
            },
            {
                "name": "APIè®¾è®¡æµ‹è¯•",
                "query": "è®¾è®¡é«˜å¹¶å‘Web APIçš„æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ",
                "expected_keywords": ["API", "å¹¶å‘", "æœ€ä½³å®è·µ", "è®¾è®¡"],
                "min_length": 300,
                "importance": "high"  
            },
            {
                "name": "è®°å¿†ç³»ç»Ÿæµ‹è¯•",
                "query": "ä½ çš„è®°å¿†ç³»ç»Ÿæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ",
                "expected_keywords": ["è®°å¿†", "ç³»ç»Ÿ", "å­˜å‚¨", "æ£€ç´¢"],
                "min_length": 200,
                "importance": "medium"
            },
            {
                "name": "ä»»åŠ¡ç®¡ç†æµ‹è¯•",
                "query": "å¸®æˆ‘åˆ›å»ºä¸€ä¸ªå­¦ä¹ AI Agentå¼€å‘çš„ä»»åŠ¡",
                "expected_keywords": ["ä»»åŠ¡", "åˆ›å»º", "AI Agent", "å­¦ä¹ "],
                "min_length": 150,
                "importance": "medium"
            }
        ]
        
        print(f"\nğŸ“Š å¼€å§‹æ‰§è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n--- æµ‹è¯• {i}/{len(test_cases)}: {test_case['name']} ---")
            
            start_time = time.time()
            
            try:
                # æ‰§è¡ŒæŸ¥è¯¢
                response = await self.agent.process_query(test_case["query"])
                
                # è®°å½•å“åº”æ—¶é—´
                response_time = time.time() - start_time
                
                # åˆ†æç»“æœ
                result = self.analyze_response(test_case, response, response_time)
                self.test_results.append(result)
                
                # æ˜¾ç¤ºç»“æœæ‘˜è¦
                self.display_test_result(result)
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
                self.test_results.append({
                    "test_name": test_case["name"],
                    "success": False,
                    "error": str(e),
                    "score": 0
                })
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡å¿«è¯·æ±‚
            await asyncio.sleep(0.5)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        await self.generate_final_report()
        
    def analyze_response(self, test_case: Dict, response, response_time: float) -> Dict:
        """åˆ†æå“åº”è´¨é‡"""
        
        result = {
            "test_name": test_case["name"],
            "query": test_case["query"], 
            "success": response.success,
            "response_time": response_time,
            "message_length": len(response.message),
            "suggestions_count": len(response.suggestions) if response.suggestions else 0,
            "next_actions_count": len(response.next_actions) if response.next_actions else 0,
            "confidence": response.data.get("confidence", 0) if response.data else 0,
            "score": 0,
            "quality_metrics": {}
        }
        
        if not response.success:
            return result
            
        score = 0
        quality_metrics = {}
        
        # 1. é•¿åº¦æ£€æŸ¥ (30åˆ†)
        if result["message_length"] >= test_case["min_length"]:
            length_score = 30
            quality_metrics["length"] = "âœ… ç¬¦åˆè¦æ±‚"
        else:
            length_score = max(0, int(30 * result["message_length"] / test_case["min_length"]))
            quality_metrics["length"] = f"âš ï¸ é•¿åº¦ä¸è¶³ ({result['message_length']}/{test_case['min_length']})"
        score += length_score
        
        # 2. å…³é”®è¯è¦†ç›– (25åˆ†)
        message_lower = response.message.lower()
        matched_keywords = [kw for kw in test_case["expected_keywords"] if kw.lower() in message_lower]
        keyword_score = int(25 * len(matched_keywords) / len(test_case["expected_keywords"]))
        score += keyword_score
        quality_metrics["keywords"] = f"âœ… {len(matched_keywords)}/{len(test_case['expected_keywords'])} å…³é”®è¯åŒ¹é…"
        
        # 3. ç»“æ„åŒ–ç¨‹åº¦ (20åˆ†)
        structure_indicators = ["**", "###", "â€¢", "-", "1.", "2.", "3.", "```"]
        structure_count = sum(1 for indicator in structure_indicators if indicator in response.message)
        structure_score = min(20, structure_count * 4)  # æœ€å¤š20åˆ†
        score += structure_score
        quality_metrics["structure"] = f"âœ… ç»“æ„åŒ–å…ƒç´ : {structure_count}"
        
        # 4. äº’åŠ¨æ€§ (15åˆ†)
        interactivity_score = 0
        if result["suggestions_count"] > 0:
            interactivity_score += 8
        if result["next_actions_count"] > 0:
            interactivity_score += 7
        score += interactivity_score
        quality_metrics["interactivity"] = f"âœ… å»ºè®®:{result['suggestions_count']}, è¡ŒåŠ¨:{result['next_actions_count']}"
        
        # 5. å“åº”é€Ÿåº¦ (10åˆ†)
        if response_time < 1.0:
            speed_score = 10
        elif response_time < 3.0:
            speed_score = 7
        elif response_time < 5.0:
            speed_score = 5
        else:
            speed_score = 2
        score += speed_score
        quality_metrics["speed"] = f"âœ… å“åº”æ—¶é—´: {response_time:.2f}ç§’"
        
        result["score"] = score
        result["quality_metrics"] = quality_metrics
        
        return result
        
    def display_test_result(self, result: Dict):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        score = result["score"]
        
        # è¯„çº§
        if score >= 90:
            grade = "ğŸ† ä¼˜ç§€"
            emoji = "ğŸ‰"
        elif score >= 80:
            grade = "â­ è‰¯å¥½"
            emoji = "ğŸ‘"
        elif score >= 70:
            grade = "âœ… åŠæ ¼"
            emoji = "ğŸ‘Œ"
        else:
            grade = "âŒ éœ€æ”¹è¿›"
            emoji = "âš ï¸"
            
        print(f"{emoji} æµ‹è¯•ç»“æœ: {grade} (å¾—åˆ†: {score}/100)")
        print(f"â±ï¸ å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
        print(f"ğŸ“ å“åº”é•¿åº¦: {result['message_length']}å­—ç¬¦")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {result['confidence']}")
        
        # æ˜¾ç¤ºè´¨é‡æŒ‡æ ‡
        for metric, description in result["quality_metrics"].items():
            print(f"   {description}")
    
    async def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ¯ SuperAgentç¬é—´ä¿®å¤æ•ˆæœéªŒè¯æŠ¥å‘Š")
        print("="*60)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r["success"])
        total_score = sum(r["score"] for r in self.test_results)
        average_score = total_score / total_tests if total_tests > 0 else 0
        average_response_time = sum(r["response_time"] for r in self.test_results) / total_tests
        
        print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸç‡: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)")
        print(f"   å¹³å‡å¾—åˆ†: {average_score:.1f}/100")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {average_response_time:.2f}ç§’")
        
        # è´¨é‡ç­‰çº§è¯„ä¼°
        if average_score >= 90:
            quality_grade = "ğŸ† å“è¶Šçº§"
            assessment = "ç¬é—´ä¿®å¤å™¨æ•ˆæœæ˜¾è‘—ï¼Œå›ç­”è´¨é‡è¾¾åˆ°ä¸“ä¸šçº§åˆ«"
        elif average_score >= 80:
            quality_grade = "â­ ä¼˜ç§€çº§"
            assessment = "ä¿®å¤æ•ˆæœè‰¯å¥½ï¼Œå¤§éƒ¨åˆ†å“åº”è¾¾åˆ°é«˜è´¨é‡æ ‡å‡†"
        elif average_score >= 70:
            quality_grade = "âœ… åˆæ ¼çº§"  
            assessment = "åŸºæœ¬è¾¾åˆ°é¢„æœŸï¼Œä»æœ‰è¿›ä¸€æ­¥ä¼˜åŒ–ç©ºé—´"
        else:
            quality_grade = "âŒ éœ€æ”¹è¿›"
            assessment = "ä¿®å¤æ•ˆæœæœ‰é™ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒä¼˜"
            
        print(f"\nğŸ–ï¸ è´¨é‡è¯„çº§: {quality_grade}")
        print(f"ğŸ’¡ è¯„ä¼°ç»“è®º: {assessment}")
        
        # è¯¦ç»†æµ‹è¯•ç»“æœè¡¨æ ¼
        print(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        print("-" * 80)
        print(f"{'æµ‹è¯•åç§°':<20} {'å¾—åˆ†':<8} {'æ—¶é—´':<8} {'é•¿åº¦':<8} {'çŠ¶æ€'}")
        print("-" * 80)
        
        for result in self.test_results:
            status = "âœ…" if result["success"] else "âŒ"
            print(f"{result['test_name']:<20} {result['score']:<8} {result['response_time']:.2f}s    {result['message_length']:<8} {status}")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_data = {
            "timestamp": time.time(),
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": successful_tests/total_tests*100,
                "average_score": average_score,
                "average_response_time": average_response_time,
                "quality_grade": quality_grade,
                "assessment": assessment
            },
            "detailed_results": self.test_results
        }
        
        report_file = Path("SuperAgent_ç¬é—´ä¿®å¤éªŒè¯æŠ¥å‘Š.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        
        # æ”¹è¿›å»ºè®®
        if average_score < 85:
            print(f"\nğŸ”§ æ”¹è¿›å»ºè®®:")
            
            # åˆ†æä¸»è¦é—®é¢˜
            low_scores = [r for r in self.test_results if r["score"] < 80]
            if low_scores:
                print(f"   â€¢ {len(low_scores)} ä¸ªæµ‹è¯•å¾—åˆ†è¾ƒä½ï¼Œå»ºè®®é’ˆå¯¹æ€§ä¼˜åŒ–")
                
            slow_responses = [r for r in self.test_results if r["response_time"] > 2.0]
            if slow_responses:
                print(f"   â€¢ {len(slow_responses)} ä¸ªæµ‹è¯•å“åº”è¾ƒæ…¢ï¼Œè€ƒè™‘æ€§èƒ½ä¼˜åŒ–")
                
            short_responses = [r for r in self.test_results if r["message_length"] < 200]
            if short_responses:
                print(f"   â€¢ {len(short_responses)} ä¸ªæµ‹è¯•å“åº”è¾ƒçŸ­ï¼Œå¯æ‰©å±•å†…å®¹æ·±åº¦")
        
        print(f"\nğŸ‰ éªŒè¯æµ‹è¯•å®Œæˆï¼Agentç¬é—´ä¿®å¤å™¨è¿è¡Œ{quality_grade}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ SuperAgentç¬é—´ä¿®å¤æ•ˆæœéªŒè¯å¼€å§‹")
    print("=" * 50)
    
    validator = AgentValidator()
    
    try:
        await validator.setup()
        await validator.run_comprehensive_test()
        
    except Exception as e:
        print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ éªŒè¯æµ‹è¯•ç»“æŸ")


if __name__ == "__main__":
    asyncio.run(main())
