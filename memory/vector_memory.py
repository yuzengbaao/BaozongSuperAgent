#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@File    : vector_memory.py
@Time    : 2025å¹´07æœˆ19æ—¥ 15:00:00
@Author  : å®æ€»
@Version : 1.0
@Desc    : å‘é‡åŒ–è®°å¿†ç³»ç»Ÿ - è¯­ä¹‰æœç´¢å¢å¼º
"""

import json
import time
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass
from pathlib import Path
import sqlite3
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    HAS_VECTOR_DEPS = True
except ImportError:
    SentenceTransformer = None
    faiss = None
    HAS_VECTOR_DEPS = False
import pickle

from .hybrid_memory_fixed import MemoryEntry, HybridMemorySystem


class VectorMemorySystem(HybridMemorySystem):
    """
    å‘é‡åŒ–å¢å¼ºçš„æ··åˆè®°å¿†ç³»ç»Ÿ
    
    åœ¨åŸæœ‰ç³»ç»ŸåŸºç¡€ä¸Šï¼Œå¢åŠ è¯­ä¹‰å‘é‡æœç´¢èƒ½åŠ›
    """
    
    def __init__(self, memory_dir: str = "./memory_storage", model_name: str = "all-MiniLM-L6-v2"):
        super().__init__(memory_dir)
        
        self.model_name = model_name
        self.embedding_model: Optional[Any] = None
        self.vector_index: Optional[Any] = None
        self.id_to_index_map: Dict[str, int] = {}
        self.index_to_id_map: Dict[int, str] = {}
        self.has_vector_support = HAS_VECTOR_DEPS
        
        if self.has_vector_support:
            self._init_vector_components()
        else:
            print("âš ï¸ å‘é‡æœç´¢ä¾èµ–æœªå®‰è£…ï¼Œä½¿ç”¨ä¼ ç»Ÿæœç´¢")
            print("ğŸ’¡ è¦å¯ç”¨è¯­ä¹‰æœç´¢ï¼Œè¯·å®‰è£…: pip install sentence-transformers faiss-cpu")
    
    def _init_vector_components(self):
        """åˆå§‹åŒ–å‘é‡ç»„ä»¶"""
        if not self.has_vector_support:
            return
            
        try:
            print("ğŸ”¥ æ­£åœ¨åŠ è½½è¯­ä¹‰å‘é‡æ¨¡å‹...")
            self.embedding_model = SentenceTransformer(self.model_name)
            
            # åˆå§‹åŒ–FAISSç´¢å¼•
            embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
            self.vector_index = faiss.IndexFlatIP(embedding_dim)  # ä½¿ç”¨å†…ç§¯ä½œä¸ºç›¸ä¼¼åº¦
            
            # åŠ è½½å·²æœ‰çš„å‘é‡ç´¢å¼•
            self._load_existing_vectors()
            
            print(f"âœ… å‘é‡ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {embedding_dim}")
            
        except Exception as e:
            print(f"âš ï¸ å‘é‡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ å°†ä½¿ç”¨åŸºç¡€æœç´¢åŠŸèƒ½")
            self.embedding_model = None
            self.vector_index = None
    
    def _load_existing_vectors(self):
        """åŠ è½½å·²æœ‰çš„å‘é‡ç´¢å¼•"""
        vector_file = self.memory_dir / "vectors.index"
        mapping_file = self.memory_dir / "vector_mappings.pkl"
        
        if vector_file.exists() and mapping_file.exists():
            try:
                self.vector_index = faiss.read_index(str(vector_file))
                with open(mapping_file, 'rb') as f:
                    mappings = pickle.load(f)
                    self.id_to_index_map = mappings['id_to_index']
                    self.index_to_id_map = mappings['index_to_id']
                
                print(f"ğŸ“š å·²åŠ è½½ {len(self.id_to_index_map)} ä¸ªå‘é‡ç´¢å¼•")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def _save_vector_index(self):
        """ä¿å­˜å‘é‡ç´¢å¼•"""
        if self.embedding_model is None:
            return
            
        try:
            vector_file = self.memory_dir / "vectors.index"
            mapping_file = self.memory_dir / "vector_mappings.pkl"
            
            faiss.write_index(self.vector_index, str(vector_file))
            
            mappings = {
                'id_to_index': self.id_to_index_map,
                'index_to_id': self.index_to_id_map
            }
            with open(mapping_file, 'wb') as f:
                pickle.dump(mappings, f)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def add_memory(
        self, 
        content: str, 
        memory_type: str = "working",
        importance: float = 0.5,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ·»åŠ è®°å¿†å¹¶åˆ›å»ºå‘é‡ç´¢å¼•"""
        entry_id = super().add_memory(content, memory_type, importance, tags, metadata)
        
        # ä¸ºæ–°è®°å¿†åˆ›å»ºå‘é‡
        if self.embedding_model is not None:
            self._add_to_vector_index(entry_id, content)
        
        return entry_id
    
    def _add_to_vector_index(self, entry_id: str, content: str):
        """æ·»åŠ å†…å®¹åˆ°å‘é‡ç´¢å¼•"""
        try:
            embedding = self.embedding_model.encode([content])
            
            # æ·»åŠ åˆ°FAISSç´¢å¼•
            current_index = self.vector_index.ntotal
            self.vector_index.add(embedding.astype(np.float32))
            
            # æ›´æ–°æ˜ å°„
            self.id_to_index_map[entry_id] = current_index
            self.index_to_id_map[current_index] = entry_id
            
            # å®šæœŸä¿å­˜ç´¢å¼•
            if len(self.id_to_index_map) % 10 == 0:
                self._save_vector_index()
                
        except Exception as e:
            print(f"âš ï¸ æ·»åŠ å‘é‡ç´¢å¼•å¤±è´¥: {e}")
    
    def semantic_search(
        self, 
        query: str, 
        max_results: int = 10,
        similarity_threshold: float = 0.7
    ) -> List[Tuple[MemoryEntry, float]]:
        """è¯­ä¹‰æœç´¢è®°å¿†"""
        if self.embedding_model is None:
            print("âš ï¸ å‘é‡æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ä¼ ç»Ÿæœç´¢")
            results = self.smart_recall(query, max_results=max_results)
            return [(entry, 1.0) for entry in results]
        
        try:
            # ç¼–ç æŸ¥è¯¢
            query_embedding = self.embedding_model.encode([query]).astype(np.float32)
            
            # æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
            scores, indices = self.vector_index.search(query_embedding, max_results * 2)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx == -1:  # FAISSè¿”å›-1è¡¨ç¤ºæ— æ•ˆç´¢å¼•
                    continue
                    
                similarity = float(score)
                if similarity < similarity_threshold:
                    continue
                
                entry_id = self.index_to_id_map.get(idx)
                if entry_id:
                    entry = self._get_memory_by_id(entry_id)
                    if entry:
                        results.append((entry, similarity))
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x[1], reverse=True)
            return results[:max_results]
            
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _get_memory_by_id(self, entry_id: str) -> Optional[MemoryEntry]:
        """æ ¹æ®IDè·å–è®°å¿†æ¡ç›®"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT id, content, timestamp, memory_type, importance, tags, metadata
            FROM memories WHERE id = ?
        """, (entry_id,))
        
        result = cursor.fetchone()
        if result:
            return MemoryEntry(
                id=result[0],
                content=result[1],
                timestamp=result[2],
                memory_type=result[3],
                importance=result[4],
                tags=json.loads(result[5]) if result[5] else [],
                metadata=json.loads(result[6]) if result[6] else {}
            )
        return None
    
    def hybrid_search(
        self, 
        query: str, 
        max_results: int = 10,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3
    ) -> List[Tuple[MemoryEntry, float]]:
        """æ··åˆæœç´¢ï¼šç»“åˆè¯­ä¹‰æœç´¢å’Œå…³é”®è¯æœç´¢"""
        
        # è¯­ä¹‰æœç´¢ç»“æœ
        semantic_results = self.semantic_search(query, max_results * 2)
        semantic_dict = {entry.id: (entry, score) for entry, score in semantic_results}
        
        # å…³é”®è¯æœç´¢ç»“æœ
        keyword_results = self.smart_recall(query, max_results=max_results * 2)
        keyword_dict = {entry.id: entry for entry in keyword_results}
        
        # åˆå¹¶ç»“æœ
        combined_results = {}
        all_ids = set(semantic_dict.keys()) | set(keyword_dict.keys())
        
        for entry_id in all_ids:
            semantic_score = semantic_dict.get(entry_id, (None, 0.0))[1]
            keyword_score = 1.0 if entry_id in keyword_dict else 0.0
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            final_score = (semantic_score * semantic_weight + 
                          keyword_score * keyword_weight)
            
            entry = (semantic_dict.get(entry_id, (None, 0))[0] or 
                    keyword_dict.get(entry_id))
            
            if entry:
                combined_results[entry_id] = (entry, final_score)
        
        # æŒ‰å¾—åˆ†æ’åºå¹¶è¿”å›
        sorted_results = sorted(
            combined_results.values(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return sorted_results[:max_results]
    
    def analyze_memory_clusters(self) -> Dict[str, Any]:
        """åˆ†æè®°å¿†èšç±»ï¼Œå‘ç°æ¨¡å¼"""
        if self.embedding_model is None or self.vector_index.ntotal == 0:
            return {"error": "å‘é‡ç´¢å¼•ä¸ºç©ºæˆ–æœªåˆå§‹åŒ–"}
        
        try:
            # è·å–æ‰€æœ‰å‘é‡
            all_vectors = self.vector_index.reconstruct_n(0, self.vector_index.ntotal)
            
            if len(all_vectors) < 2:
                return {"clusters": 0, "message": "è®°å¿†æ¡ç›®å¤ªå°‘ï¼Œæ— æ³•èšç±»"}
            
            # ç®€å•çš„k-meansèšç±»
            from sklearn.cluster import KMeans
            import warnings
            warnings.filterwarnings("ignore")
            
            n_clusters = min(5, len(all_vectors) // 3)  # åŠ¨æ€ç¡®å®šèšç±»æ•°é‡
            if n_clusters < 2:
                n_clusters = 2
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(all_vectors)
            
            # åˆ†ææ¯ä¸ªèšç±»
            clusters = {}
            for i, label in enumerate(cluster_labels):
                entry_id = self.index_to_id_map.get(i)
                if entry_id:
                    entry = self._get_memory_by_id(entry_id)
                    if entry:
                        if label not in clusters:
                            clusters[label] = []
                        clusters[label].append({
                            "content": entry.content[:100] + "...",
                            "type": entry.memory_type,
                            "importance": entry.importance
                        })
            
            return {
                "clusters": len(clusters),
                "cluster_info": clusters,
                "total_memories": len(all_vectors)
            }
            
        except Exception as e:
            return {"error": f"èšç±»åˆ†æå¤±è´¥: {e}"}
    
    def close(self):
        """å…³é—­ç³»ç»Ÿæ—¶ä¿å­˜å‘é‡ç´¢å¼•"""
        self._save_vector_index()
        super().close()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸš€ å®æ€»çš„å‘é‡åŒ–è®°å¿†ç³»ç»Ÿæµ‹è¯•å¼€å§‹ï¼")
    
    # åˆ›å»ºå‘é‡è®°å¿†ç³»ç»Ÿ
    vector_memory = VectorMemorySystem()
    
    # æ·»åŠ æµ‹è¯•æ•°æ®
    test_memories = [
        ("Pythonæ˜¯å®æ€»æœ€å–œæ¬¢çš„ç¼–ç¨‹è¯­è¨€", "semantic", 0.9, ["ç¼–ç¨‹", "åå¥½"]),
        ("LangChainæ˜¯æ„å»ºAIåº”ç”¨çš„å¼ºå¤§æ¡†æ¶", "knowledge", 0.8, ["AI", "æ¡†æ¶"]),
        ("ä»Šå¤©å®Œæˆäº†æ··åˆè®°å¿†ç³»ç»Ÿçš„è®¾è®¡", "episodic", 0.8, ["é¡¹ç›®", "å®Œæˆ"]),
        ("éœ€è¦ä¼˜åŒ–å‘é‡æœç´¢çš„æ€§èƒ½", "task", 0.6, ["ä¼˜åŒ–", "æ€§èƒ½"]),
        ("Agentéœ€è¦å…·å¤‡é•¿æœŸè®°å¿†èƒ½åŠ›", "insight", 0.9, ["AI", "è®°å¿†"])
    ]
    
    for content, mem_type, importance, tags in test_memories:
        vector_memory.add_memory(content, mem_type, importance, tags)
    
    # æµ‹è¯•è¯­ä¹‰æœç´¢
    print("\nğŸ” è¯­ä¹‰æœç´¢æµ‹è¯•:")
    semantic_results = vector_memory.semantic_search("äººå·¥æ™ºèƒ½å¼€å‘", max_results=3)
    for entry, score in semantic_results:
        print(f"  ç›¸ä¼¼åº¦ {score:.3f}: {entry.content}")
    
    # æµ‹è¯•æ··åˆæœç´¢
    print("\nğŸ¯ æ··åˆæœç´¢æµ‹è¯•:")
    hybrid_results = vector_memory.hybrid_search("Pythonå¼€å‘", max_results=3)
    for entry, score in hybrid_results:
        print(f"  ç»¼åˆå¾—åˆ† {score:.3f}: {entry.content}")
    
    # èšç±»åˆ†æ
    print("\nğŸ“Š è®°å¿†èšç±»åˆ†æ:")
    cluster_info = vector_memory.analyze_memory_clusters()
    print(json.dumps(cluster_info, indent=2, ensure_ascii=False))
    
    vector_memory.close()
    print("\nâœ… å‘é‡è®°å¿†ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
