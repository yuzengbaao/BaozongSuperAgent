#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@File    : memory_enhancer.py
@Time    : 2025å¹´07æœˆ19æ—¥ 15:30:00
@Author  : å®æ€»
@Version : 1.0
@Desc    : è®°å¿†ç³»ç»Ÿå¢å¼ºå™¨ - æ™ºèƒ½åˆ†æå’Œä¸Šä¸‹æ–‡ç®¡ç†
"""

import json
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter, defaultdict
from pathlib import Path

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from hybrid_memory_fixed import HybridMemorySystem, MemoryEntry


class MemoryEnhancer(HybridMemorySystem):
    """
    è®°å¿†ç³»ç»Ÿå¢å¼ºå™¨
    
    åœ¨åŸºç¡€æ··åˆè®°å¿†ç³»ç»Ÿä¸Šæ·»åŠ æ™ºèƒ½åˆ†æå’Œä¸Šä¸‹æ–‡ç®¡ç†åŠŸèƒ½
    """
    
    def __init__(self, memory_dir: str = "./memory_storage"):
        super().__init__(memory_dir)
        
        self.conversation_session_id: str = ""
        self.session_memories: List[MemoryEntry] = []
        self.knowledge_graph: Dict[str, Dict[str, Any]] = {}
        
        self._init_enhancer()
    
    def _init_enhancer(self):
        """åˆå§‹åŒ–å¢å¼ºå™¨ç»„ä»¶"""
        self._load_knowledge_graph()
        self.conversation_session_id = f"session_{int(time.time())}"
        print(f"ğŸš€ è®°å¿†å¢å¼ºå™¨å¯åŠ¨ï¼Œä¼šè¯ID: {self.conversation_session_id}")
    
    def _load_knowledge_graph(self):
        """åŠ è½½çŸ¥è¯†å›¾è°±"""
        kg_file = self.memory_dir / "knowledge_graph.json"
        if kg_file.exists():
            try:
                with open(kg_file, 'r', encoding='utf-8') as f:
                    self.knowledge_graph = json.load(f)
                print(f"ğŸ“Š å·²åŠ è½½ {len(self.knowledge_graph)} ä¸ªçŸ¥è¯†èŠ‚ç‚¹")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
                self.knowledge_graph = {}
        else:
            self.knowledge_graph = {}
    
    def _save_knowledge_graph(self):
        """ä¿å­˜çŸ¥è¯†å›¾è°±"""
        kg_file = self.memory_dir / "knowledge_graph.json"
        try:
            with open(kg_file, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_graph, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
    
    def add_memory_with_analysis(
        self,
        content: str,
        memory_type: str = "working",
        importance: float = 0.5,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        auto_analyze: bool = True
    ) -> str:
        """æ·»åŠ è®°å¿†å¹¶è¿›è¡Œæ™ºèƒ½åˆ†æ"""
        
        # è‡ªåŠ¨åˆ†æå†…å®¹
        if auto_analyze:
            analyzed_tags, analyzed_importance, analyzed_metadata = self._analyze_content(content)
            
            # åˆå¹¶åˆ†æç»“æœ
            final_tags = list(set((tags or []) + analyzed_tags))
            final_importance = max(importance, analyzed_importance)
            final_metadata = {**(metadata or {}), **analyzed_metadata}
        else:
            final_tags = tags
            final_importance = importance
            final_metadata = metadata
        
        # æ·»åŠ ä¼šè¯ä¿¡æ¯
        if final_metadata is None:
            final_metadata = {}
        final_metadata["session_id"] = self.conversation_session_id
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•æ·»åŠ è®°å¿†
        entry_id = super().add_memory(
            content, memory_type, final_importance, final_tags, final_metadata
        )
          # æ›´æ–°çŸ¥è¯†å›¾è°±
        if final_tags:
            self._update_knowledge_graph(content, final_tags)
        
        # æ·»åŠ åˆ°ä¼šè¯è®°å¿†
        entry = MemoryEntry(
            id=entry_id,
            content=content,
            timestamp=time.time(),
            memory_type=memory_type,
            importance=final_importance,
            tags=final_tags,
            metadata=final_metadata
        )
        self.session_memories.append(entry)
        
        return entry_id
    
    def _analyze_content(self, content: str) -> Tuple[List[str], float, Dict[str, Any]]:
        """åˆ†æå†…å®¹ï¼Œæå–æ ‡ç­¾ã€è¯„ä¼°é‡è¦æ€§å’Œå…ƒæ•°æ®"""
        tags = []
        importance = 0.5
        metadata = {}
        
        content_lower = content.lower()
        
        # æŠ€æœ¯æ ‡ç­¾è¯†åˆ«
        tech_patterns = {
            "python": r"\bpython\b",
            "ai": r"\b(ai|artificial intelligence|äººå·¥æ™ºèƒ½|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ )\b",
            "langchain": r"\blangchain\b",
            "agent": r"\bagent\b",
            "è®°å¿†": r"\b(è®°å¿†|memory|è®°ä½)\b",
            "é¡¹ç›®": r"\b(é¡¹ç›®|project|ä»»åŠ¡|task)\b",
            "å¼€å‘": r"\b(å¼€å‘|development|ç¼–ç¨‹|coding)\b",
            "æ¡†æ¶": r"\b(æ¡†æ¶|framework|åº“|library)\b"
        }
        
        for tag, pattern in tech_patterns.items():
            if re.search(pattern, content_lower):
                tags.append(tag)
        
        # é‡è¦æ€§è¯„ä¼°
        importance_indicators = {
            r"\b(é‡è¦|å…³é”®|æ ¸å¿ƒ|critical|important)\b": 0.3,
            r"\b(å®Œæˆ|æˆåŠŸ|success|å®Œæ¯•)\b": 0.2,
            r"\b(é”™è¯¯|å¤±è´¥|é—®é¢˜|error|bug)\b": 0.2,
            r"\b(ä¼˜åŒ–|æ”¹è¿›|æå‡|optimize)\b": 0.1,
            r"\b(å­¦ä¹ |ç ”ç©¶|åˆ†æ|study)\b": 0.1
        }
        
        for pattern, weight in importance_indicators.items():
            if re.search(pattern, content_lower):
                importance += weight
        
        importance = min(importance, 1.0)
        
        # æƒ…æ„Ÿåˆ†æ (ç®€å•ç‰ˆæœ¬)
        positive_words = ["å¥½", "æ£’", "æˆåŠŸ", "å®Œæˆ", "ä¼˜ç§€", "æ»¡æ„", "great", "good", "success"]
        negative_words = ["é—®é¢˜", "é”™è¯¯", "å¤±è´¥", "å›°éš¾", "bug", "error", "fail", "difficult"]
        
        positive_count = sum(1 for word in positive_words if word in content_lower)
        negative_count = sum(1 for word in negative_words if word in content_lower)
        
        if positive_count > negative_count:
            metadata["sentiment"] = "positive"
        elif negative_count > positive_count:
            metadata["sentiment"] = "negative"
        else:
            metadata["sentiment"] = "neutral"
        
        # å†…å®¹ç±»å‹è¯†åˆ«
        if any(word in content_lower for word in ["å®Œæˆ", "å®ç°", "åšäº†", "åˆ›å»º"]):
            metadata["action_type"] = "achievement"
        elif any(word in content_lower for word in ["éœ€è¦", "è¦", "è®¡åˆ’", "æ‰“ç®—"]):
            metadata["action_type"] = "plan"
        elif any(word in content_lower for word in ["å­¦åˆ°", "å‘ç°", "äº†è§£", "çŸ¥é“"]):
            metadata["action_type"] = "learning"
        
        return tags, importance, metadata
    
    def _update_knowledge_graph(self, content: str, tags: List[str]):
        """æ›´æ–°çŸ¥è¯†å›¾è°±"""
        if not tags:
            return
        
        # ä¸ºæ¯ä¸ªæ ‡ç­¾åˆ›å»ºæˆ–æ›´æ–°èŠ‚ç‚¹
        for tag in tags:
            if tag not in self.knowledge_graph:
                self.knowledge_graph[tag] = {
                    "count": 0,
                    "related_tags": {},
                    "first_seen": time.time(),
                    "last_seen": time.time(),
                    "importance": 0.0
                }
            
            # æ›´æ–°èŠ‚ç‚¹ä¿¡æ¯
            node = self.knowledge_graph[tag]
            node["count"] += 1
            node["last_seen"] = time.time()
            node["importance"] = min(1.0, node["importance"] + 0.1)
            
            # æ›´æ–°å…³è”å…³ç³»
            for other_tag in tags:
                if other_tag != tag:
                    if other_tag not in node["related_tags"]:
                        node["related_tags"][other_tag] = 0
                    node["related_tags"][other_tag] += 1
        
        # å®šæœŸä¿å­˜çŸ¥è¯†å›¾è°±
        if len(self.session_memories) % 5 == 0:
            self._save_knowledge_graph()
    
    def get_smart_context(self, query: str, max_context_items: int = 8) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = super().get_context_for_agent(query)
        
        # æ·»åŠ å¢å¼ºä¿¡æ¯
        context.update({
            "session_id": self.conversation_session_id,
            "session_memories_count": len(self.session_memories),
            "knowledge_insights": self._get_knowledge_insights(query),
            "memory_patterns": self._analyze_memory_patterns(),
            "suggested_tags": self._suggest_tags_for_query(query)
        })
        
        return context
    
    def _get_knowledge_insights(self, query: str) -> Dict[str, Any]:
        """æ ¹æ®æŸ¥è¯¢è·å–çŸ¥è¯†æ´å¯Ÿ"""
        query_lower = query.lower()
        relevant_nodes = {}
        
        for tag, node_info in self.knowledge_graph.items():
            if tag.lower() in query_lower:
                relevant_nodes[tag] = {
                    "frequency": node_info["count"],
                    "importance": node_info["importance"],
                    "related_concepts": list(node_info["related_tags"].keys())[:3]
                }
        
        return {
            "relevant_concepts": relevant_nodes,
            "total_concepts": len(self.knowledge_graph)
        }
    
    def _analyze_memory_patterns(self) -> Dict[str, Any]:
        """åˆ†æè®°å¿†æ¨¡å¼"""
        if not self.session_memories:
            return {"pattern": "no_data"}
        
        # åˆ†æè®°å¿†ç±»å‹åˆ†å¸ƒ
        type_distribution = Counter([mem.memory_type for mem in self.session_memories])
        
        # åˆ†ææ—¶é—´æ¨¡å¼
        recent_hours = [
            datetime.fromtimestamp(mem.timestamp).hour 
            for mem in self.session_memories
            if time.time() - mem.timestamp < 24 * 3600
        ]
        active_hours = Counter(recent_hours).most_common(3)
        
        # åˆ†æé‡è¦æ€§åˆ†å¸ƒ
        importance_levels = ["ä½", "ä¸­", "é«˜"]
        importance_dist = {
            "ä½": len([m for m in self.session_memories if m.importance < 0.4]),
            "ä¸­": len([m for m in self.session_memories if 0.4 <= m.importance < 0.7]),
            "é«˜": len([m for m in self.session_memories if m.importance >= 0.7])
        }
        
        return {
            "memory_types": dict(type_distribution),
            "active_hours": [{"hour": h, "count": c} for h, c in active_hours],
            "importance_distribution": importance_dist,
            "session_length": len(self.session_memories)
        }
    
    def _suggest_tags_for_query(self, query: str) -> List[str]:
        """ä¸ºæŸ¥è¯¢å»ºè®®ç›¸å…³æ ‡ç­¾"""
        query_lower = query.lower()
        suggestions = []
        
        # åŸºäºçŸ¥è¯†å›¾è°±å»ºè®®
        for tag, node_info in self.knowledge_graph.items():
            if tag.lower() in query_lower:
                # æ·»åŠ ç›¸å…³æ ‡ç­¾
                related = sorted(
                    node_info["related_tags"].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:3]
                suggestions.extend([tag for tag, _ in related])
        
        return list(set(suggestions))
    
    def generate_session_summary(self) -> str:
        """ç”Ÿæˆä¼šè¯æ€»ç»“"""
        if not self.session_memories:
            return "æœ¬æ¬¡ä¼šè¯æš‚æ— è®°å¿†è®°å½•ã€‚"
        
        total_memories = len(self.session_memories)
        high_importance = len([m for m in self.session_memories if m.importance >= 0.7])
        
        # æœ€å¸¸è§çš„æ ‡ç­¾
        all_tags = []
        for mem in self.session_memories:
            if mem.tags:
                all_tags.extend(mem.tags)
        
        common_tags = Counter(all_tags).most_common(5)
        
        # æœ€æ–°çš„å‡ æ¡é‡è¦è®°å¿†
        important_memories = sorted(
            [m for m in self.session_memories if m.importance >= 0.6],
            key=lambda x: x.timestamp,
            reverse=True
        )[:3]
        
        summary = f"""
# ä¼šè¯æ€»ç»“ ({self.conversation_session_id})

## åŸºæœ¬ç»Ÿè®¡
- æ€»è®°å¿†æ•°: {total_memories}
- é‡è¦è®°å¿†æ•°: {high_importance}
- ä¼šè¯æ—¶é•¿: {self._format_duration(time.time() - self.session_memories[0].timestamp if self.session_memories else 0)}

## ä¸»è¦è¯é¢˜
{', '.join([f"{tag}({count})" for tag, count in common_tags])}

## é‡è¦è®°å¿†ç‰‡æ®µ
"""
        
        for i, mem in enumerate(important_memories, 1):
            summary += f"{i}. {mem.content[:100]}...\n"
        
        return summary
    
    def _format_duration(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´é•¿åº¦"""
        if seconds < 60:
            return f"{int(seconds)}ç§’"
        elif seconds < 3600:
            return f"{int(seconds//60)}åˆ†é’Ÿ"
        else:
            return f"{int(seconds//3600)}å°æ—¶{int((seconds%3600)//60)}åˆ†é’Ÿ"
    
    def close(self):
        """å…³é—­æ—¶ä¿å­˜æ•°æ®"""
        self._save_knowledge_graph()
        
        # ä¿å­˜ä¼šè¯æ€»ç»“
        if self.session_memories:
            summary_file = self.memory_dir / f"session_summary_{self.conversation_session_id}.md"
            try:
                with open(summary_file, 'w', encoding='utf-8') as f:
                    f.write(self.generate_session_summary())
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜ä¼šè¯æ€»ç»“å¤±è´¥: {e}")
        
        super().close()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ¯ å®æ€»çš„è®°å¿†å¢å¼ºå™¨æµ‹è¯•å¼€å§‹ï¼")
    
    # åˆ›å»ºè®°å¿†å¢å¼ºå™¨
    enhancer = MemoryEnhancer()
    
    # æ·»åŠ æµ‹è¯•è®°å¿†å¹¶åˆ†æ
    test_memories = [
        "ä»Šå¤©å®Œæˆäº†Pythoné¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½å¼€å‘ï¼Œæ„Ÿè§‰å¾ˆæ£’ï¼",
        "éœ€è¦ä¼˜åŒ–AI Agentçš„è®°å¿†ç³»ç»Ÿæ€§èƒ½ï¼Œè¿™æ˜¯ä¸ªé‡è¦ä»»åŠ¡",
        "å­¦ä¹ äº†LangChainæ¡†æ¶çš„ä½¿ç”¨æ–¹æ³•ï¼Œå¾ˆæœ‰ç”¨",
        "å‘ç°äº†ä¸€ä¸ªå…³é”®bugï¼Œéœ€è¦ç´§æ€¥ä¿®å¤",
        "æˆåŠŸå®ç°äº†æ··åˆè®°å¿†ç³»ç»Ÿï¼Œå®æ€»å¾ˆæ»¡æ„"
    ]
    
    for i, content in enumerate(test_memories):
        memory_id = enhancer.add_memory_with_analysis(
            content,
            memory_type="episodic" if i % 2 == 0 else "task",
            auto_analyze=True
        )
        print(f"æ·»åŠ è®°å¿†: {memory_id}")
    
    # æµ‹è¯•æ™ºèƒ½ä¸Šä¸‹æ–‡
    print("\nğŸ§  æ™ºèƒ½ä¸Šä¸‹æ–‡æµ‹è¯•:")
    context = enhancer.get_smart_context("Pythonå¼€å‘é¡¹ç›®")
    print(f"çŸ¥è¯†æ´å¯Ÿ: {context['knowledge_insights']}")
    print(f"è®°å¿†æ¨¡å¼: {context['memory_patterns']}")
    print(f"å»ºè®®æ ‡ç­¾: {context['suggested_tags']}")
    
    # ç”Ÿæˆä¼šè¯æ€»ç»“
    print("\nğŸ“‹ ä¼šè¯æ€»ç»“:")
    print(enhancer.generate_session_summary())
    
    enhancer.close()
    print("\nâœ… è®°å¿†å¢å¼ºå™¨æµ‹è¯•å®Œæˆï¼")
